import os
from typing import List
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

# Charger la clÃ© API OpenAI depuis le fichier .env
load_dotenv()

# Initialisation du modÃ¨le LLM
llm = ChatOpenAI(
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    model="gpt-4o",
    temperature=0
)

def generate_recommendations(anomalies: List[str]) -> str:
 
    prompt = f"""
    Tu es un expert en architecture technique et optimisation de performance.

    Voici une liste dâ€™anomalies dÃ©tectÃ©es :
    {anomalies}

    Pour chaque anomalie, fournis une recommandation technique prÃ©cise.
    Formate la rÃ©ponse comme un JSON de ce type :

    {{
        "anomalies": [
            {{
                "description": "CPU usage is too high.",
                "recommendation": "RÃ©partir la charge sur plusieurs machines ou augmenter les ressources CPU."
            }}
        ]
    }}
    """

    messages = [HumanMessage(content=prompt)]
    result = llm.invoke(messages)
    return result.content

# Test local direct
if __name__ == "__main__":
    test_anomalies = [
        "CPU usage is too high.",
        "Latency is too high.",
        "API Gateway is degraded."
    ]
    print("ğŸ“‹ Anomalies simulÃ©es :", test_anomalies)
    print("\nğŸ§  Appel au LLM pour recommandations...")
    recommendations = generate_recommendations(test_anomalies)
    print("\nâœ… Rapport gÃ©nÃ©rÃ© :\n")
    print(recommendations)
